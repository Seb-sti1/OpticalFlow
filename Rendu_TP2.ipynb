{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Rendu du TP2 de Billy WEYNANS et Sébastien KERBOURC'H\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "### À cause de l'utilisation d'OpenCV, **il est préférable d'ouvrir ce rendu avec un IDE** tel que *PyCharm* ou *VS Code*.\n",
    "Les fenêtres ouvertes par OpenCV ne se comportent pas correctement lors de l'exécution via ```jupyter notebook file.ipynb```\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "### Pour fermer les fenêtres OpenCV, il faut utiliser q xou échap.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "os.chdir(\"src\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "e05eca73",
   "metadata": {},
   "source": [
    "# Histogramme de couleurs\n",
    "\n",
    "### Q1 Histogramme Yuv pour détection de changement de plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('../videos/Extrait1-Cosmos_Laundromat1(340p).m4v')\n",
    "\n",
    "# Convention opencv\n",
    "# Si entier alors entre 0 et 255\n",
    "# Si float entre 0 et 1\n",
    "\n",
    "# Check if the video file was successfully loaded\n",
    "if not cap.isOpened():\n",
    "    print(\"Error loading video file\")\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read the next frame from the video file\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    frameYuv = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n",
    "    # Don't forget the [] around the frame\n",
    "    hist = cv2.calcHist([frameYuv], [1,2], None, [256, 256], [0,255, 0, 255])\n",
    "\n",
    "    # Log of histogram\n",
    "    hist = np.log(hist)\n",
    "\n",
    "    # Normalization of histogram\n",
    "    hist = hist/ hist.max() * 255\n",
    "\n",
    "    # Conversion for opencv\n",
    "    hist = hist.astype(np.uint8)\n",
    "\n",
    "    # Colormap because it's cool\n",
    "    hist = cv2.applyColorMap(hist, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Check if there are no more frames\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('histogramme', hist)\n",
    "\n",
    "    # Wait for a key press\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video file and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On remarque des changements soudains lors des changements de plan alors que les scènes ont des changements progressifs de leur histogramme.\n",
    "\n",
    "On pourrait calculer l'auto-corrélation entre l'histogramme courant et le précédent.\n",
    "La valeur de cette auto-corrélation serait ensuite comparée à un seuil prédéfini et lorsque l'auto-corrélation dépasse ce seuil, on détecterait un changement de plan."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Histogramme de vitesses\n",
    "\n",
    "### Q2 Méthode de Färneback pour estimation du flux optique\n",
    "\n",
    "La méthode de Färneback utilise 2 images pour donner une estimation dense du flux optique. Pour chaque image i, un découpage en zone est réalisé et chaque entourage de chaque point est estimée par une expression polynomiale matricielle notée f_i(x). La forme des polynomes étant connue, on peut ainsi remonter au déplacement d (qui est un déplacement sur une zone entière et paramétré à composantes polynomiales) en minimisant une somme d'erreur pondérée d'écarts entre f_2(x) et f_1(x-d) pour chaque point de la zone.\n",
    "Ainsi, le déplacement d calculé est le déplacement minimisant l'erreur sur une zone entière.\n",
    "\n",
    "Afin d'affiner l'estimation du flux optique, la méthode est réalisée sur plusieurs résolutions d'images et itérée sur des zones de plus en plus petites pour chaque résolution.\n",
    "\n",
    "Au début, la résolution est faible (grande échelle) afin d'obtenir un déplacement moins fin mais qui permet de calculer de grands déplacements (donc de grandes vitesses) et qui apportera de la connaissance a priori pour les itérations et niveaux suivants.\n",
    "\n",
    "\n",
    "Les paramètres de la fonction cv2.calcOpticalFlowFarneback sont :\n",
    "- prvs : Image 1\n",
    "- next : Image 2\n",
    "- pyr_scale : Taux de réduction de l'échelle entre les niveaux de la pyramide d'images (càd la collection d'images créées à partir de l'image initiale en changeant l'échelle)\n",
    "    - Un pyr_scale trop bas donne des zones bruitées\n",
    "    - Peu d'amélioration au-delà de 0.5\n",
    "- levels : Nombre de niveaux de la pyramide\n",
    "    - Plus levels est petit, moins les résultats seront stables\n",
    "- winsize : Taille de zone pour le calcul de d\n",
    "    - L'augmenter rend le flux optique plus grossier\n",
    "    - Le mettre à 1 fait apparaître les résultats bruités dont Färneback parle dans son papier\n",
    "- iterations : Nombre d'iterations pour chaque niveau de la pyramide\n",
    "    - Augmenter iterations donne de meilleurs contours au flux optique\n",
    "    - Peu d'améliorations au-delà de 3\n",
    "- poly_n : Taille du voisinage pour un pixel\n",
    "    - Augmenter poly_n semble rendre le flux optique plus flou donc plus lisse\n",
    "- poly_sigma : Écart-type du filtre gaussien utilisé pour calculer les dérivées\n",
    "    - Augmenter poly_sigma réduit la capacité de détection des mouvements rapides\n",
    "    - Réduire poly_sigma semble donner un flux optique plus homogène\n",
    "- flags : Options supplémentaires permettant de customiser la fonction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q3 Histogramme de vitesse pour détermination de type de plan\n",
    "\n",
    "Voici le code permettant de calculer et afficher l'histogramme 2D de la probabilité jointe des composantes (Vx, Vy) du flot optique."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Ouverture du flux video\n",
    "cap = cv2.VideoCapture(\"../videos/Extrait5-Matrix-Helicopter_Scene(280p).m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Rotation_OX(Tilt).m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Rotation_OY(Pan).m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Rotation_OZ(Roll).m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/ZOOM_O_TRAVELLING.m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Travelling_OX.m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Travelling_OZ.m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Extrait3-Vertigo-Dream_Scene(320p).m4v\")\n",
    "# cap = cv2.VideoCapture('../videos/Extrait1-Cosmos_Laundromat1(340p).m4v')\n",
    "ret, frame1 = cap.read()  # Passe à l'image suivante\n",
    "\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)  # Passage en niveaux de gris\n",
    "hsv = np.zeros_like(frame1)  # Image nulle de même taille que frame1 (affichage OF)\n",
    "hsv[:, :, 1] = 255  # Toutes les couleurs sont saturées au maximum\n",
    "\n",
    "index = 1\n",
    "ret, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while (ret):\n",
    "    index += 1\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None,\n",
    "                                        pyr_scale=0.5,  # Taux de réduction pyramidal\n",
    "                                        levels=3,  # Nombre de niveaux de la pyramide\n",
    "                                        winsize=15,\n",
    "                                        # Taille de fenêtre de lissage (moyenne) des coefficients polynomiaux\n",
    "                                        iterations=3,  # Nb d'itérations par niveau\n",
    "                                        poly_n=7,  # Taille voisinage pour approximation polynomiale\n",
    "                                        poly_sigma=1.5,  # E-T Gaussienne pour calcul dérivé\n",
    "                                        flags=0)\n",
    "\n",
    "    # L'histogramme utilise -flow et [1, 0] au lieu de [0, 1] afin de faire correspondre visuellement les vitesses\n",
    "    # (Points nombreux à gauche dans l'histogramme quand vitesse de l'image vers la gauche etc)\n",
    "    histr = cv2.calcHist([-flow[:, :, 0], -flow[:, :, 1]], [1, 0], None, [512, 512], [-1, 1, -1, 1])\n",
    "\n",
    "    histr = np.log(histr)/np.log(histr.max())*255\n",
    "    histr[histr == -np.inf] = 0\n",
    "    histr = histr.astype(np.uint8)\n",
    "\n",
    "\n",
    "    mag, ang = cv2.cartToPolar(flow[:, :, 0], flow[:, :, 1])  # Conversion cartésien vers polaire\n",
    "    hsv[:, :, 0] = (ang * 180) / (2 * np.pi)  # Teinte (codée sur [0..179] dans OpenCV) <--> Argument\n",
    "    hsv[:, :, 2] = (mag * 255) / np.amax(mag)  # Valeur <--> Norme\n",
    "\n",
    "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    result = np.vstack((frame2, bgr))\n",
    "    cv2.imshow('Image et Champ de vitesses (Farneback)', result)\n",
    "\n",
    "    histr = cv2.applyColorMap(histr, cv2.COLORMAP_JET)\n",
    "\n",
    "    cv2.imshow('Histo', histr)\n",
    "\n",
    "    k = cv2.waitKey(15) & 0xff\n",
    "    if k == 27:  # escape\n",
    "        break\n",
    "    elif k == ord('s'):  # or index == 100:\n",
    "        cv2.imwrite('Frame_%04d_%d.png' % (index, time.time()), frame2)\n",
    "        cv2.imwrite('OF_hsv_%04d_%d.png' % (index, time.time()), bgr)\n",
    "\n",
    "    prvs = next\n",
    "    ret, frame2 = cap.read()\n",
    "    if ret:\n",
    "        next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Après visualisation de quelques extraits, on remarque plusieurs types de scène reconnaissables par l'histogramme des vitesses :\n",
    "\n",
    "- Plan fixe : Vitesses centrées autour de 0 avec un écart-type a priori faible\n",
    "- Panoramique :\n",
    "    - Horizontal : Vy autour de 0 et Vx regroupées autour d'une valeur moyenne non nulle\n",
    "    - Vertical : Vx autour de 0 et Vy regroupées autour d'une valeur moyenne non nulle\n",
    "- Rotation : Histogramme très homogène, pas de plage de valeur particulière de Vx et Vy -> faible entropie ?\n",
    "- Travelling : Vx et Vy sont plus largement distribuées dans l'histogramme\n",
    "- Zoom : Très similaire au travelling avant/arrière\n",
    "\n",
    "Ainsi, on a déterminé quelques métriques associées à l'histogramme 2d qui pourraient être pertinentes pour comparer les types de plan :\n",
    "- Valeurs max et min selon x et y\n",
    "- Valeurs moyennes selon x et y\n",
    "- Entropies selon x et y (faible = homogène)\n",
    "- Mode (couple de vitesses majoritaire)\n",
    "- Amplitude selon x et y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Découpage et indexation\n",
    "\n",
    "### Q4 Découpage en plans\n",
    "\n",
    "Pour rappel, l'idée première était de calculer l'auto-corrélation entre l'histogramme courant et le précédent car on remarque des changements soudains lors des changements de plan alors que les scènes ont des changements progressifs de leur histogramme.\n",
    "La valeur de cette auto-corrélation serait ensuite comparée à un seuil prédéfini et lorsque l'auto-corrélation dépasse ce seuil, on détecterait un changement de plan.\n",
    "\n",
    "Un premier essai a été réalisé en utilisant la somme des différences absolues (SAD) pour comparer les histogrammes. Cette méthode détecte les changements de plan, mais également les changements rapides de couleur.\n",
    "\n",
    "Comme il ne semble pas possible de définir une valeur de threshold fixe efficace l'idée est venue de calculer une valeur de threshold en fonction des n derniers histogrammes afin de s'adapter à la scène en cours.\n",
    "\n",
    "Le threshold est calculé comme suivant : threshold = threshold_gain*(moyenne des SAD des nb_hists derniers histogrammes)\n",
    "\n",
    "threshold_gain permet donc de contrôler la sensibilité de détection de changement de scène.\n",
    "\n",
    "Les changements de plan sont notés par un point rouge en haut à gauche de l'image de base et un bip sonore (fonctionnel en fonction .\n",
    "Voici le code :"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n",
      "Changement de plan       SAD :  8453098.0       threshold 0.0         SAD/threshold inf\n",
      "\u0007\n",
      "Changement de plan       SAD :  326479.0       threshold 193780.8         SAD/threshold 1.6847850767465096\n",
      "\u0007\n",
      "Changement de plan       SAD :  370591.0       threshold 238576.5         SAD/threshold 1.5533424289483666\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Ouverture du flux video\n",
    "cap = cv2.VideoCapture(\"../videos/Extrait5-Matrix-Helicopter_Scene(280p).m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Rotation_OX(Tilt).m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Rotation_OY(Pan).m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Rotation_OZ(Roll).m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/ZOOM_O_TRAVELLING.m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Travelling_OX.m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Travelling_OZ.m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Extrait3-Vertigo-Dream_Scene(320p).m4v\")\n",
    "# cap = cv2.VideoCapture('../videos/Extrait1-Cosmos_Laundromat1(340p).m4v')\n",
    "\n",
    "# Convention opencv\n",
    "# Si entier alors entre 0 et 255\n",
    "# Si float entre 0 et 1\n",
    "\n",
    "# Paramètres du cercle indicateur de changement de plan\n",
    "center = (50, 50)  # Coordonnées (x, y) du centre du cercle\n",
    "radius = 25  # Rayon du cercle\n",
    "color = (0, 0, 255)  # Couleur du cercle en format BGR (rouge)\n",
    "thickness = -1  # Épaisseur du cercle (-1 pour remplir le cercle)\n",
    "\n",
    "# Check if the video file was successfully loaded\n",
    "if not cap.isOpened():\n",
    "    print(\"Error loading video file\")\n",
    "\n",
    "nb_hists = 5\n",
    "hist_idx = 0  # Keep track of idx of hist to add the last one in the array of hists with minimal computation\n",
    "threshold_gain = 4.5\n",
    "hists_array = np.zeros(shape=(nb_hists, 256, 256, 3))\n",
    "SAD = 0\n",
    "\n",
    "hist_idx = 0\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read the next frame from the video file\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    frameYuv = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n",
    "\n",
    "    # Don't forget the [] around the frame\n",
    "    hist = cv2.calcHist([frameYuv], [1, 2], None, [256, 256], [0, 255, 0, 255])\n",
    "    # Log\n",
    "    hist = np.log(hist)\n",
    "    # Normalization\n",
    "    hist = hist / hist.max() * 255\n",
    "    hist = hist.astype(np.uint8)\n",
    "    hist = cv2.applyColorMap(hist, cv2.COLORMAP_JET)\n",
    "\n",
    "    # Sum of Absolute Differencies of the current histogram\n",
    "    SAD = np.sum(np.abs(hist - hists_array[hist_idx - 1]))\n",
    "\n",
    "    # init threshold to 0\n",
    "    threshold = 0\n",
    "\n",
    "    # Sum of Absolute Differencies to compare the current and previous histograms\n",
    "    for i in range(1, nb_hists):\n",
    "        threshold += np.sum(np.abs(hists_array[i] - hists_array[i - 1]))\n",
    "    # Adjustment of the threshold with threshold_gain\n",
    "    threshold = threshold_gain * threshold / nb_hists\n",
    "\n",
    "    if SAD > threshold:\n",
    "        print('\\a')  # Fait un bip sonore\n",
    "        frame = cv2.circle(frame, center, radius, color, thickness)\n",
    "        print(\"Changement de plan       SAD : \", SAD, \"      threshold\", threshold, \"        SAD/threshold\",\n",
    "              SAD / threshold)\n",
    "\n",
    "    # Add the current histogram to the list for the upcoming histogram of the next iteration\n",
    "    hists_array[hist_idx] = hist\n",
    "    # Update the hist_idx to access the latest histogram\n",
    "    hist_idx = hist_idx + 1 if (hist_idx < nb_hists - 1) else 0\n",
    "\n",
    "    # Check if there are no more frames\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('histogramme', hist)\n",
    "\n",
    "    # Wait for a key press\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video file and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On a commencé les essais à threshold_gain = 2, ce qui a donné :\n",
    "- L'amélioration la plus flagrante : les changements abondants et progressifs (bien que rapides) ne sont plus détectés. L'exemple le plus marquant est l'apparition dans le ciel de la tornade de couleur.\n",
    "- Certains des éclairs de la scène sont souvent détectés comme des changements de scène. C'est logique car tous les pixels deviennent blancs donc la colorimétrie de la scène change brutalement et c'est ce que détecte le code créé.\n",
    "\n",
    "Cependant, après quelques ajustements pour trouver le bon threshold_gain, le programme détecte la grande majorité des changements de plan mais toujours environ la moitié des éclairs dans l'extrait *Cosmos Laundromat*.\n",
    "\n",
    "Voici certains problèmes liés à cette méthode :\n",
    "- Les transitions trop douces ne sont pas correctement détectées, comme dans l'extrait *Vertigo Dream Scene*.\n",
    "- Certains changements trop soudains dans la colorimétrie peuvent être détectés comme changement de plan\n",
    "\n",
    "En niveaux de gris, on peut utiliser un histogramme 1D et réaliser exactement le même traitement. Cela a été réalisé sur l'extrait 2 *Man With A Movie Camera* et a fonctionné de façon très correcte."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Q5 Identification de la frame principale\n",
    "\n",
    "Afin de rester dans l'esprit des histogrammes, notre première idée est d'identifier la frame décrivant le mieux la scène -que l'on appellera la frame principale- à l'aide des maxima de l'entropie sur un plan car l'entropie permet de mesurer l'homogénéité de la répartition des vitesses dans l'image.\n",
    "\n",
    "Après des tests sur quelques extraits, la frame identifiée par le max de l'entropie sur un plan semblait plus pertinente la majorité du temps. C'est donc cette méthode qui a été retenue.\n",
    "\n",
    "Elle n'est cependant pas parfaite car elle tient uniquement compte des vitesses et de leur homogénéité donc l'image-clef extraite se base uniquement sur le mouvement or on comprend bien que c'est loin d'être l'unique caractéristique d'une image. L'objectif pour correctement identifier une image-clef serait de pouvoir extraire du contexte à partir de la vidéo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Q6 Identification de type de plan\n",
    "\n",
    "Initialement, l'idée était de comparer les métriques déterminées en Q3 entre elles afin de déterminer le type de plan. Par exemple, si ```(Vx_mean/Vy_mean) >= threshold``` alors on a un certain type de plan et ainsi de suite pour chaque type de plan. Cela a donné l'ébauche horrifique suivante :\n",
    "&nbsp;\n",
    "\n",
    "```python\n",
    "    # Plan fixe\n",
    "    if Vx_abs_max <= thr_vabs_fixe and Vy_abs_max <= thr_vabs_fixe and Vx_mean <= thr_mean_fixe and Vy_mean <= thr_mean_fixe:\n",
    "        return 'plan fixe'\n",
    "    # Zoom et travelling avant/arrière\n",
    "    elif Vx_amplitude/Vy_amplitude<=rapport_amplitudes_zoom and Vx_mean/Vy_mean <= rapport_mean_zoom:\n",
    "        return 'zoom' #TODO déterminer avant ou arrière\n",
    "\n",
    "    # Travelling horizontal\n",
    "    elif np.abs(Vx_mean/Vy_mean) >= rapport_mean_trav_h:\n",
    "        return 'travelling horizontal'\n",
    "    elif np.abs(Vx_mean/Vy_mean) >= rapport_mean_trav_v:\n",
    "        return 'travelling vertical'\n",
    "    return 'indéterminé'\n",
    "```\n",
    "&nbsp;\n",
    "\n",
    "Regarder chaque vidéo et trouver des relations entre les métriques uniquement vérifiées dans un type de plan précis se révéla assez pénible et peu fructueux.\n",
    "J'ai donc voulu appliquer ce que j'ai appris en cours de machine learning et de tenter d'utiliser un Support Vector Classifier ou un k-NN Classifier avec Scikit Learn.\n",
    "\n",
    "Afin de trouver le meilleur modèle parmi SVC ou k-NN avec les meilleurs paramètres, j'ai utilisé GridSearch puis sauvegardé le modèle avec la bibliothèque ```joblib```.\n",
    "Le code complet se trouve dans ```Grid-Search-Svm-Or-Knn.py```.\n",
    "\n",
    "Les données d'entrainement ont été générées avec ```Data-Maker.py```. Cette version modifiée de ```Dense-Optical-Flow.py``` permet d'annoter l'image courante via les touches du keypad (un numéro par classe) et d'enregistrer les données dans ```data.csv```.\n",
    "\n",
    "Finalement le classifieur k-NN était le plus performant d'après plusieurs tests. Voici les paramètres et une moyenne des performances :\n",
    "```\n",
    "Meilleurs hyperparamètres k-NN : {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'uniform'}\n",
    "Meilleur score de validation croisée k-NN : 0.7036363636363636\n",
    "Score sur l'ensemble de test : 0.75\n",
    "```\n",
    "\n",
    "Voici le code permettant d'identifer le type de plan :\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import Metrics\n",
    "\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "# Chargement du k-NN entrainé\n",
    "model_path = '../models/knn_plan.joblib'\n",
    "knn = joblib.load(model_path)\n",
    "type_plans = ['plan fixe', 'pano horizontal', 'pano vertical', 'rotation', 'trav horizontal', 'trav vertical',\n",
    "              'trav avant', 'trav arriere', 'zoom avant', 'zoom arriere']\n",
    "X_columns = [\"Vx_entropy\", \"Vy_entropy\", \"Vx_amplitude\", \"Vy_amplitude\", \"Vx_max\", \"Vy_max\", \"Vx_min\", \"Vy_min\",\n",
    "             \"Vx_mean\", \"Vy_mean\"]\n",
    "\n",
    "# Paramètres pour affichage du type de plan\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX  # Police du texte\n",
    "font_scale = 1  # Échelle de la police (taille du texte)\n",
    "color = (255, 255, 255)  # Couleur du texte (B, G, R)\n",
    "thickness = 2  # Épaisseur des lignes du texte\n",
    "\n",
    "# Ouverture du flux video\n",
    "# cap = cv2.VideoCapture(\"../videos/Extrait5-Matrix-Helicopter_Scene(280p).m4v\")\n",
    "cap = cv2.VideoCapture(\"../videos/Rotation_OX(Tilt).m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Rotation_OY(Pan).m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Rotation_OZ(Roll).m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/ZOOM_O_TRAVELLING.m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Travelling_OX.m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Travelling_OZ.m4v\")\n",
    "# cap = cv2.VideoCapture(\"../videos/Extrait3-Vertigo-Dream_Scene(320p).m4v\")\n",
    "# cap = cv2.VideoCapture('../videos/Extrait1-Cosmos_Laundromat1(340p).m4v')\n",
    "\n",
    "ret, frame1 = cap.read()  # Passe à l'image suivante\n",
    "\n",
    "if frame1 is None:\n",
    "    print(\"Erreur\")\n",
    "\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)  # Passage en niveaux de gris\n",
    "hsv = np.zeros_like(frame1)  # Image nulle de même taille que frame1 (affichage OF)\n",
    "hsv[:, :, 1] = 255  # Toutes les couleurs sont saturées au maximum\n",
    "index = 1\n",
    "ret, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while (ret):\n",
    "    index += 1\n",
    "\n",
    "    # Renvoie numpy.ndarray a la même taille que les images d'entrée et contient les coordonnées du vecteur\n",
    "    # de mouvement (u, v) pour chaque pixel. Les coordonnées du vecteur sont stockées sous forme de canaux,\n",
    "    # où le premier canal (u) représente le déplacement horizontal et le deuxième canal (v) représente le déplacement vertical\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None,\n",
    "                                        pyr_scale=0.5,  # Taux de réduction pyramidal\n",
    "                                        levels=5,  # Nombre de niveaux de la pyramide\n",
    "                                        winsize=15,\n",
    "                                        # Taille de fenêtre de lissage (moyenne) des coefficients polynomiaux\n",
    "                                        iterations=3,  # Nb d'itérations par niveau\n",
    "                                        poly_n=5,  # Taille voisinage pour approximation polynomiale\n",
    "                                        poly_sigma=0.5,  # E-T Gaussienne pour calcul dérivé\n",
    "                                        flags=0)\n",
    "\n",
    "    # L'histogramme utilise flow et [1, 0] au lieu de [0, 1] afin de faire correspondre visuellement les vitesses\n",
    "    # (Points nombreux à gauche dans l'histogramme quand vitesse de l'image négative etc)\n",
    "    histr = cv2.calcHist([flow[:, :, 0], flow[:, :, 1]], [1, 0], None, [512, 512], [-1, 1, -1, 1])\n",
    "\n",
    "    X = Metrics.get_X_vector(flow, histr)  # Calcul à partir de l'histogramme et du flow par facilité des formules\n",
    "    X = pd.DataFrame(data=[X], columns=X_columns)\n",
    "    type_plan = type_plans[knn.predict(X)[0]] if not X.isna().any().any() else \" \"\n",
    "\n",
    "    histr = np.log(histr) / np.log(histr.max()) * 255\n",
    "    histr[histr == -np.inf] = 0\n",
    "    histr = histr.astype(np.uint8)\n",
    "\n",
    "    mag, ang = cv2.cartToPolar(flow[:, :, 0], flow[:, :, 1])  # Conversion cartésien vers polaire\n",
    "    hsv[:, :, 0] = (ang * 180) / (2 * np.pi)  # Teinte (codée sur [0..179] dans OpenCV) <--> Argument\n",
    "    hsv[:, :, 2] = (mag * 255) / np.amax(mag)  # Valeur <--> Norme\n",
    "\n",
    "    bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    result = np.vstack((frame2, bgr))\n",
    "    result = cv2.putText(result, type_plan, org=(frame1.shape[0] // 2, frame1.shape[1] // 2 + 50), fontFace=font,\n",
    "                         fontScale=font_scale, color=color, thickness=thickness)\n",
    "\n",
    "    histr = cv2.applyColorMap(histr, cv2.COLORMAP_JET)\n",
    "\n",
    "    cv2.imshow('Histo', histr)\n",
    "    cv2.imshow('Image et Champ de vitesses (Farneback)', result)\n",
    "\n",
    "    k = cv2.waitKey(15) & 0xff\n",
    "    if k == 27:  # escape\n",
    "        break\n",
    "    elif k == ord('s'):  # or index == 100:\n",
    "        cv2.imwrite('Frame_%04d_%d.png' % (index, time.time()), frame2)\n",
    "        cv2.imwrite('OF_hsv_%04d_%d.png' % (index, time.time()), bgr)\n",
    "\n",
    "    prvs = next\n",
    "    ret, frame2 = cap.read()\n",
    "    if ret:\n",
    "        next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "À noter que cette méthode estime le type de plan pour chaque frame or on veut identifier le type de plan sur un plan entier détecté automatiquement.\n",
    "Ainsi, un système de votes par frame est mis en place et le type de plan identifié sera celui avec le plus de votes sur un plan donné.\n",
    "&nbsp;\n",
    "\n",
    "Finalement, on peut émettre quelques remarques :\n",
    "- La technique marche correctement même si cela est probablement biaisé par le fait que les données d'entraînement sont issues des données d'utilisation finales.\n",
    "    - Le dataset de train est très petit (77 vecteurs) car c'est plutôt la mise en oeuvre qui m'importait dans ce TP\n",
    "- Le modèle confond souvent zoom avant (et arrière) et travelling avant (et arrière) lorsqu'on estime le type de plan sur une image du flot. Cependant, le système de vote majoritaire corrige ceci et le modèle ne confond pas ces types de plan sur un plan entier."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# La combinaison des méthodes montrées jusqu'ici est essayable en exécutant la cellule suivante :\n",
    "&nbsp;\n",
    "### Le script python va déterminer et afficher une image descriptive pour chaque plan détecté de l'extrait choisi. L'image sera affichée dans une fenêtre à part en-dessous de l'image potentiellement principale du plan en cours."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wnbilly/Cours/mi204/OpticalFlow/src\n",
      "\u0007\n",
      "\u0007\n",
      "\u0007\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Au cas où vous n'auriez pas exécuté les cellules nécessaires\n",
    "import os\n",
    "# os.chdir(\"src\")\n",
    "\n",
    "''' Pour essayer avec différentes vidéos\n",
    "\"Extrait5-Matrix-Helicopter_Scene(280p).m4v\"\n",
    "\"Rotation_OX(Tilt).m4v\"\n",
    "\"Rotation_OY(Pan).m4v\"\n",
    "\"Rotation_OZ(Roll).m4v\"\n",
    "\"ZOOM_O_TRAVELLING.m4v\"\n",
    "\"Travelling_OX.m4v\"\n",
    "\"Travelling_OZ.m4v\"\n",
    "\"videos/Extrait3-Vertigo-Dream_Scene(320p).m4v\"\n",
    "\"videos/Extrait1-Cosmos_Laundromat1(340p).m4v\"\n",
    "'''\n",
    "\n",
    "file_name = 'ZOOM_O_TRAVELLING.m4v'\n",
    "DISPLAY_HISTOGRAMS = 0 # 0 to disable display of uv and Vx Vy histograms\n",
    "DISPLAY_FLOW = 1 # 0 to disable display of flow\n",
    "DISPLAY_MAIN_FRAME = 1 # 0 to disable display of the main frame of the shot\n",
    "SAVE_MAIN_FRAMES = 0 # 0 to disable the saving of the main frames as .png\n",
    "\n",
    "print(os.getcwd())\n",
    "os.system(f\"python3 Scene-Splitter.py '{file_name}' {DISPLAY_HISTOGRAMS} {DISPLAY_FLOW} {DISPLAY_MAIN_FRAME} {SAVE_MAIN_FRAMES}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
